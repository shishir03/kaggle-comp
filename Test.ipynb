{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1710670\n",
      "320\n"
     ]
    }
   ],
   "source": [
    "# Load the training data\n",
    "train_data = pd.read_csv('archive/train.csv')\n",
    "test_data=pd.read_csv('archive/test_public.csv')\n",
    "print(len(train_data))\n",
    "print(len(test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "# Drop irrelevant columns (TRIP_ID, TAXI_ID, etc.)\n",
    "# Encode categorical variables (CALL_TYPE, DAY_TYPE)\n",
    "# Handle missing values (ORIGIN_CALL, ORIGIN_STAND)\n",
    "\n",
    "#Doesn't actually work right now\n",
    "#train_data.drop(train_data[train_data['MISSING_DATA'].to_string() == 'True'].index, inplace = True)\n",
    "\n",
    "preprocessed_data = train_data.drop(['TRIP_ID', 'DAY_TYPE', 'MISSING_DATA'], axis=1)\n",
    "\n",
    "preprocessed_data = pd.get_dummies(preprocessed_data, columns=['CALL_TYPE'])\n",
    "preprocessed_data['CALL_TYPE'] = preprocessed_data['CALL_TYPE_A'].astype(int) + preprocessed_data['CALL_TYPE_B'].astype(int) * 2 + preprocessed_data['CALL_TYPE_C'].astype(int) * 3\n",
    "preprocessed_data = preprocessed_data.drop(['CALL_TYPE_A', 'CALL_TYPE_B', 'CALL_TYPE_C'], axis=1)\n",
    "\n",
    "\n",
    "preprocessed_data['ORIGIN_CALL'].fillna(0, inplace=True)\n",
    "preprocessed_data['ORIGIN_STAND'].fillna(0, inplace=True)\n",
    "\n",
    "preprocessed_data['TAXI_ID'] = preprocessed_data['TAXI_ID'] - 20000000  # Normalize taxi IDs\n",
    "\n",
    "preprocessed_data['TIMESTAMP'] = pd.to_datetime(preprocessed_data['TIMESTAMP'], unit='s')\n",
    "preprocessed_data['HOUR'] = preprocessed_data['TIMESTAMP'].dt.hour\n",
    "preprocessed_data['DAY_OF_WEEK'] = preprocessed_data['TIMESTAMP'].dt.dayofweek + 1\n",
    "preprocessed_data['WEEK_OF_YEAR'] = preprocessed_data['TIMESTAMP'].dt.isocalendar().week.astype(int)\n",
    "preprocessed_data = preprocessed_data.drop('TIMESTAMP', axis=1)\n",
    "\n",
    "X = preprocessed_data.drop(['POLYLINE'], axis=1)\n",
    "y = (train_data['POLYLINE'].str.count(',')-1) * 15  # Calculate travel time in seconds\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the threshold for the top 5% travel time\n",
    "top_percentile = np.percentile(y_train, 90)\n",
    "\n",
    "# Filter the data based on the travel time threshold\n",
    "X_train = X_train[y_train <= top_percentile]\n",
    "y_train = y_train[y_train <= top_percentile]\n",
    "\n",
    "# Repeat the filtering for the validation set\n",
    "X_val = X_val[y_val <= top_percentile]\n",
    "y_val = y_val[y_val <= top_percentile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400.0\n",
      "         ORIGIN_CALL  ORIGIN_STAND  TAXI_ID  CALL_TYPE  HOUR  DAY_OF_WEEK  \\\n",
      "1566255       3998.0           0.0      296          1    16            1   \n",
      "795817           0.0          20.0      392          2    20            3   \n",
      "1144137          0.0           0.0       39          3     7            4   \n",
      "180789           0.0          36.0      597          2    12            5   \n",
      "479959           0.0          42.0      285          2    22            6   \n",
      "...              ...           ...      ...        ...   ...          ...   \n",
      "259178       24662.0           0.0      184          1    18            3   \n",
      "1414414          0.0          37.0       83          2    10            7   \n",
      "131932           0.0          25.0      600          2    19            7   \n",
      "671155           0.0          25.0      451          2    16            5   \n",
      "121958           0.0           0.0      342          2    14            5   \n",
      "\n",
      "         WEEK_OF_YEAR  \n",
      "1566255            23  \n",
      "795817             51  \n",
      "1144137            10  \n",
      "180789             32  \n",
      "479959             41  \n",
      "...               ...  \n",
      "259178             35  \n",
      "1414414            18  \n",
      "131932             30  \n",
      "671155             47  \n",
      "121958             30  \n",
      "\n",
      "[1232149 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(top_percentile)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-28 {color: black;background-color: white;}#sk-container-id-28 pre{padding: 0;}#sk-container-id-28 div.sk-toggleable {background-color: white;}#sk-container-id-28 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-28 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-28 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-28 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-28 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-28 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-28 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-28 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-28 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-28 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-28 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-28 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-28 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-28 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-28 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-28 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-28 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-28 div.sk-item {position: relative;z-index: 1;}#sk-container-id-28 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-28 div.sk-item::before, #sk-container-id-28 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-28 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-28 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-28 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-28 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-28 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-28 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-28 div.sk-label-container {text-align: center;}#sk-container-id-28 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-28 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-28\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.15, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=8, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=200, n_jobs=None, num_parallel_tree=None,\n",
       "             predictor=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-28\" type=\"checkbox\" checked><label for=\"sk-estimator-id-28\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.15, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=8, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=200, n_jobs=None, num_parallel_tree=None,\n",
       "             predictor=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.15, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=8, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=200, n_jobs=None, num_parallel_tree=None,\n",
       "             predictor=None, random_state=None, ...)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = xgb.XGBRegressor(n_estimators=300,max_depth=8,learning_rate=0.15)\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1922038.4755723134\n",
      "RMSE: 1386.376022431257\n",
      "Inputs with the biggest losses:\n",
      "ORIGIN_CALL       0.0\n",
      "ORIGIN_STAND      0.0\n",
      "TAXI_ID         562.0\n",
      "CALL_TYPE         3.0\n",
      "HOUR              7.0\n",
      "DAY_OF_WEEK       6.0\n",
      "WEEK_OF_YEAR     20.0\n",
      "Name: 1492417, dtype: float64\n",
      "____\n",
      "\n",
      "ORIGIN_CALL       0.0\n",
      "ORIGIN_STAND      0.0\n",
      "TAXI_ID         902.0\n",
      "CALL_TYPE         3.0\n",
      "HOUR             16.0\n",
      "DAY_OF_WEEK       3.0\n",
      "WEEK_OF_YEAR     31.0\n",
      "Name: 147121, dtype: float64\n",
      "____\n",
      "\n",
      "ORIGIN_CALL       0.0\n",
      "ORIGIN_STAND      0.0\n",
      "TAXI_ID         510.0\n",
      "CALL_TYPE         3.0\n",
      "HOUR             12.0\n",
      "DAY_OF_WEEK       5.0\n",
      "WEEK_OF_YEAR     24.0\n",
      "Name: 1627045, dtype: float64\n",
      "____\n",
      "\n",
      "ORIGIN_CALL       0.0\n",
      "ORIGIN_STAND      0.0\n",
      "TAXI_ID         665.0\n",
      "CALL_TYPE         3.0\n",
      "HOUR              8.0\n",
      "DAY_OF_WEEK       3.0\n",
      "WEEK_OF_YEAR     46.0\n",
      "Name: 633224, dtype: float64\n",
      "____\n",
      "\n",
      "ORIGIN_CALL       0.0\n",
      "ORIGIN_STAND      0.0\n",
      "TAXI_ID         520.0\n",
      "CALL_TYPE         3.0\n",
      "HOUR             18.0\n",
      "DAY_OF_WEEK       3.0\n",
      "WEEK_OF_YEAR     33.0\n",
      "Name: 205445, dtype: float64\n",
      "____\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "y_pred = model.predict(X_val)\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"RMSE: {np.sqrt(mse)}\")\n",
    "\n",
    "# Calculate absolute differences\n",
    "abs_diff = np.abs(y_val - y_pred)\n",
    "\n",
    "# Find indices of inputs with the biggest losses\n",
    "top_indices = np.argsort(abs_diff)[-5:][::-1]\n",
    "\n",
    "# Print the inputs with the biggest losses\n",
    "print(\"Inputs with the biggest losses:\")\n",
    "for i in top_indices:\n",
    "    print(X_val.iloc[i])\n",
    "    print(\"____\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10981362 0.21117005 0.1868946  0.16885234 0.14956258 0.08111721\n",
      " 0.09258962]\n",
      "         ORIGIN_CALL  ORIGIN_STAND  TAXI_ID  CALL_TYPE  HOUR  DAY_OF_WEEK  \\\n",
      "1414161          0.0          10.0      213          2     7            7   \n",
      "1397309          0.0           0.0       92          3     7            4   \n",
      "560966           0.0           9.0      263          2    14            2   \n",
      "1453634          0.0          13.0      503          2    21            6   \n",
      "889280           0.0           0.0      678          3    14            2   \n",
      "...              ...           ...      ...        ...   ...          ...   \n",
      "1064661          0.0           0.0      367          3    15            1   \n",
      "1195435          0.0           0.0      547          3    16            1   \n",
      "1661714          0.0          56.0      692          2     7            6   \n",
      "545123           0.0          14.0      648          2     2            6   \n",
      "1215152      52782.0           0.0      621          1     9            6   \n",
      "\n",
      "         MONTH  \n",
      "1414161      5  \n",
      "1397309      5  \n",
      "560966      10  \n",
      "1453634      5  \n",
      "889280       1  \n",
      "...        ...  \n",
      "1064661      2  \n",
      "1195435      3  \n",
      "1661714      6  \n",
      "545123      10  \n",
      "1215152      3  \n",
      "\n",
      "[325496 rows x 7 columns]\n",
      "[1066.3217 1360.0272 1495.5183 ... 1120.0779 1055.4681 1266.0446]\n"
     ]
    }
   ],
   "source": [
    "print(model.feature_importances_)\n",
    "print(X_val)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "# Drop irrelevant columns (TRIP_ID, TAXI_ID, etc.)\n",
    "# Encode categorical variables (CALL_TYPE, DAY_TYPE)\n",
    "# Handle missing values (ORIGIN_CALL, ORIGIN_STAND)\n",
    "trip_ids = test_data['TRIP_ID']\n",
    "preprocessed_test_data = test_data.drop(['TRIP_ID', 'DAY_TYPE', 'MISSING_DATA'], axis=1)\n",
    "\n",
    "preprocessed_test_data = pd.get_dummies(preprocessed_test_data, columns=['CALL_TYPE'])\n",
    "preprocessed_test_data['CALL_TYPE'] = preprocessed_test_data['CALL_TYPE_A'].astype(int) + preprocessed_test_data['CALL_TYPE_B'].astype(int) * 2 + preprocessed_test_data['CALL_TYPE_C'].astype(int) * 3\n",
    "preprocessed_test_data = preprocessed_test_data.drop(['CALL_TYPE_A', 'CALL_TYPE_B', 'CALL_TYPE_C'], axis=1)\n",
    "\n",
    "preprocessed_test_data['ORIGIN_CALL'].fillna(0, inplace=True)\n",
    "preprocessed_test_data['ORIGIN_STAND'].fillna(0, inplace=True)\n",
    "\n",
    "preprocessed_test_data['TAXI_ID'] = preprocessed_test_data['TAXI_ID'] - 20000000  # Normalize taxi IDs\n",
    "\n",
    "preprocessed_test_data['TIMESTAMP'] = pd.to_datetime(preprocessed_test_data['TIMESTAMP'], unit='s')\n",
    "preprocessed_test_data['HOUR'] = preprocessed_test_data['TIMESTAMP'].dt.hour\n",
    "preprocessed_test_data['DAY_OF_WEEK'] = preprocessed_test_data['TIMESTAMP'].dt.dayofweek + 1\n",
    "preprocessed_test_data['WEEK_OF_YEAR'] = preprocessed_test_data['TIMESTAMP'].dt.isocalendar().week.astype(int)\n",
    "preprocessed_test_data = preprocessed_test_data.drop('TIMESTAMP', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "results = model.predict(preprocessed_test_data)\n",
    "\n",
    "fields = ['TRIP_ID', 'TRAVEL_TIME']\n",
    "rows = []\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    row = [str(trip_ids[i]), result]\n",
    "    rows.append(row)\n",
    "\n",
    "with open(\"submission.csv\", 'w') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(fields)\n",
    "    csvwriter.writerows(rows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
