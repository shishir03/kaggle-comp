{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data length: 1710670 test_data length: 320\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_data = pd.read_csv('archive/train.csv')\n",
    "test_data=pd.read_csv('archive/test_public.csv')\n",
    "print(\"train_data length:\",len(train_data), \"test_data length:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ORIGIN_CALL  ORIGIN_STAND  TAXI_ID  CALL_TYPE  HOUR  DAY_OF_WEEK  \\\n",
      "1547701          0.0           0.0      112          3    10            5   \n",
      "1515924          0.0          15.0      525          2    18            5   \n",
      "344382           0.0          12.0      100          2    14            1   \n",
      "660091           0.0           0.0      353          3     9            3   \n",
      "986649           0.0          27.0      128          2    11            5   \n",
      "...              ...           ...      ...        ...   ...          ...   \n",
      "952058           0.0          37.0      591          2     2            4   \n",
      "54791            0.0           1.0      107          2    19            4   \n",
      "1159806          0.0           0.0      503          3     7            7   \n",
      "1095899      57773.0           0.0      346          1     6            1   \n",
      "1490210          0.0           0.0      126          3    12            7   \n",
      "\n",
      "         WEEK_OF_YEAR  \n",
      "1547701            22  \n",
      "1515924            21  \n",
      "344382             38  \n",
      "660091             47  \n",
      "986649              5  \n",
      "...               ...  \n",
      "952058              4  \n",
      "54791              28  \n",
      "1159806            10  \n",
      "1095899             9  \n",
      "1490210            20  \n",
      "\n",
      "[500000 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess Data\n",
    "train_data = train_data[train_data['MISSING_DATA'] != True] #Drop any rows with missing data\n",
    "preprocessed_data = train_data.drop(['TRIP_ID', 'DAY_TYPE', 'MISSING_DATA'], axis=1) #Drop irrelevant columns\n",
    "\n",
    "preprocessed_data = pd.get_dummies(preprocessed_data, columns=['CALL_TYPE'])\n",
    "preprocessed_data['CALL_TYPE'] = preprocessed_data['CALL_TYPE_A'].astype(int) + preprocessed_data['CALL_TYPE_B'].astype(int) * 2 + preprocessed_data['CALL_TYPE_C'].astype(int) * 3\n",
    "preprocessed_data = preprocessed_data.drop(['CALL_TYPE_A', 'CALL_TYPE_B', 'CALL_TYPE_C'], axis=1)\n",
    "\n",
    "preprocessed_data['ORIGIN_CALL'].fillna(0, inplace=True)\n",
    "preprocessed_data['ORIGIN_STAND'].fillna(0, inplace=True)\n",
    "preprocessed_data['TAXI_ID'] = preprocessed_data['TAXI_ID'] - 20000000 #Normalize taxi IDs\n",
    "\n",
    "preprocessed_data['TIMESTAMP'] = pd.to_datetime(preprocessed_data['TIMESTAMP'], unit='s')\n",
    "preprocessed_data['HOUR'] = preprocessed_data['TIMESTAMP'].dt.hour\n",
    "preprocessed_data['DAY_OF_WEEK'] = preprocessed_data['TIMESTAMP'].dt.dayofweek + 1\n",
    "preprocessed_data['WEEK_OF_YEAR'] = preprocessed_data['TIMESTAMP'].dt.isocalendar().week.astype(int)\n",
    "preprocessed_data = preprocessed_data.drop('TIMESTAMP', axis=1)\n",
    "\n",
    "def polyline_to_trip_duration(polyline):\n",
    "  return max(polyline.count(\"[\") - 2, 0) * 15\n",
    "\n",
    "# This code creates a new column, \"LEN\", in our dataframe. The value is\n",
    "# the (polyline_length - 1) * 15, where polyline_length = count(\"[\") - 1\n",
    "preprocessed_data[\"LEN\"] = preprocessed_data[\"POLYLINE\"].apply(polyline_to_trip_duration)\n",
    "\n",
    "train, val = train_test_split(preprocessed_data, test_size=0.2, random_state=42)\n",
    "train = train.sample(500000)\n",
    "X_train = train.drop(['POLYLINE', 'LEN'], axis=1)\n",
    "y_train = train['LEN']  # Calculate travel time in seconds\n",
    "X_val = val.drop(['POLYLINE', 'LEN'], axis=1)\n",
    "y_val = val['LEN']  # Calculate travel time in seconds\n",
    "\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Calculate the threshold for the top 1% travel time\n",
    "top_percentile = np.percentile(y_train, 99)\n",
    "\n",
    "# Filter the data based on the travel time threshold\n",
    "X_train = X_train[y_train <= top_percentile]\n",
    "y_train = y_train[y_train <= top_percentile]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "[CV] END ........................................subsample=0; total time=   3.2s\n",
      "[CV] END ........................................subsample=0; total time=   3.3s\n",
      "[CV] END ........................................subsample=0; total time=   3.2s\n",
      "[CV] END ......................................subsample=0.3; total time=  10.8s\n",
      "[CV] END ......................................subsample=0.3; total time=  10.9s\n",
      "[CV] END ......................................subsample=0.3; total time=  11.7s\n",
      "[CV] END ......................................subsample=0.6; total time=  13.4s\n",
      "[CV] END ......................................subsample=0.6; total time=  13.5s\n",
      "[CV] END ......................................subsample=0.6; total time=  12.3s\n",
      "{'subsample': 0.6}\n",
      "Mean Squared Error: 442414.4959784708\n",
      "RMSE: 665.142462919389\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    #'n_estimators': [100, 300, 500],       300\n",
    "    #'max_depth': [4,6,8,10],               4 (5 seems slightly better)\n",
    "    #'learning_rate': [0.05,0.15,0.3],      0.15\n",
    "    #'gamma': [0, 0.5, 1, 2],               IRRELEVANT\n",
    "    #'colsample_bytree': [0.5,0.75,1],      0.5 (0.75 seems slightly better)\n",
    "    #'min_child_weight': [1, 5, 10]         IRRELEVANT\n",
    "}\n",
    "\n",
    "model = xgb.XGBRegressor(learning_rate=0.15, n_estimators=300, max_depth=4, colsample_bytree=0.5)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "print(best_params)\n",
    "\n",
    "y_pred = best_model.predict(X_val)\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"RMSE: {np.sqrt(mse)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 433833.78632658976\n",
      "RMSE: 658.6606002537193\n"
     ]
    }
   ],
   "source": [
    "model = xgb.XGBRegressor(n_estimators=300,max_depth=5,learning_rate=0.15,colsample_bytree=0.75)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_val)\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"RMSE: {np.sqrt(mse)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06173237 0.10853538 0.18023314 0.30610937 0.1566261  0.11687596\n",
      " 0.06988759]\n",
      "716.35706 691.9835 195.5302\n",
      "717.5000876854548 600.0 694.5655365840385\n"
     ]
    }
   ],
   "source": [
    "# Full data: mean 580 median 584 std 97\n",
    "# 100000: mean 583 median 587 std 111\n",
    "# 50000: mean 579 median 582 std 117\n",
    "# 10000: mean 583 median 585 std 147\n",
    "\n",
    "print(model.feature_importances_)\n",
    "print(y_pred.mean(), np.median(y_pred), y_pred.std())\n",
    "print(y_val.mean(), np.median(y_val), y_val.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess testing data\n",
    "trip_ids = test_data['TRIP_ID']\n",
    "preprocessed_test_data = test_data.drop(['TRIP_ID', 'DAY_TYPE', 'MISSING_DATA'], axis=1)\n",
    "\n",
    "preprocessed_test_data = pd.get_dummies(preprocessed_test_data, columns=['CALL_TYPE'])\n",
    "preprocessed_test_data['CALL_TYPE'] = preprocessed_test_data['CALL_TYPE_A'].astype(int) + preprocessed_test_data['CALL_TYPE_B'].astype(int) * 2 + preprocessed_test_data['CALL_TYPE_C'].astype(int) * 3\n",
    "preprocessed_test_data = preprocessed_test_data.drop(['CALL_TYPE_A', 'CALL_TYPE_B', 'CALL_TYPE_C'], axis=1)\n",
    "\n",
    "preprocessed_test_data['ORIGIN_CALL'].fillna(0, inplace=True)\n",
    "preprocessed_test_data['ORIGIN_STAND'].fillna(0, inplace=True)\n",
    "\n",
    "preprocessed_test_data['TAXI_ID'] = preprocessed_test_data['TAXI_ID'] - 20000000  # Normalize taxi IDs\n",
    "\n",
    "preprocessed_test_data['TIMESTAMP'] = pd.to_datetime(preprocessed_test_data['TIMESTAMP'], unit='s')\n",
    "preprocessed_test_data['HOUR'] = preprocessed_test_data['TIMESTAMP'].dt.hour\n",
    "preprocessed_test_data['DAY_OF_WEEK'] = preprocessed_test_data['TIMESTAMP'].dt.dayofweek + 1\n",
    "preprocessed_test_data['WEEK_OF_YEAR'] = preprocessed_test_data['TIMESTAMP'].dt.isocalendar().week.astype(int)\n",
    "preprocessed_test_data = preprocessed_test_data.drop('TIMESTAMP', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write results to submission.csv\n",
    "results = model.predict(preprocessed_test_data)\n",
    "fields = ['TRIP_ID', 'TRAVEL_TIME']\n",
    "rows = []\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    row = [str(trip_ids[i]), result]\n",
    "    rows.append(row)\n",
    "\n",
    "with open(\"submission.csv\", 'w') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(fields)\n",
    "    csvwriter.writerows(rows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
